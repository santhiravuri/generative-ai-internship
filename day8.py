# -*- coding: utf-8 -*-
"""day8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TEKIDlLPD8HwTfvp7V0WfqCDJ0I6huHE
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torchvision.utils import save_image, make_grid
from torch.utils.data import DataLoader
import os

IMAGE_SIZE = 64
CHANNELS_IMG = 3
Z_DIM = 100
BATCH_SIZE = 128
NUM_EPOCHS = 50
LR = 2e-4  #0.0002
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# âœ… Save directory for Colab
SAMPLE_DIR = "/content/samples"
os.makedirs(SAMPLE_DIR, exist_ok=True)

class Generator(nn.Module):
    def __init__(self, z_dim):
        super(Generator, self).__init__()
        self.gen = nn.Sequential(
            nn.ConvTranspose2d(z_dim, 512, 4, 1, 0),
            nn.BatchNorm2d(512),
            nn.ReLU(True),

            nn.ConvTranspose2d(512, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(True),

            nn.ConvTranspose2d(64, CHANNELS_IMG, 4, 2, 1),
            nn.Tanh()
        )

    def forward(self, x):
        return self.gen(x)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.disc = nn.Sequential(
            nn.Conv2d(CHANNELS_IMG, 64, 4, 2, 1),
            nn.LeakyReLU(0.2),

            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),

            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),

            nn.Conv2d(256, 512, 4, 2, 1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2),

            nn.Conv2d(512, 1, 4, 1, 0),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.disc(x)

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm") != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

transform = transforms.Compose([
    transforms.Resize(IMAGE_SIZE),
    transforms.CenterCrop(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

dataloader = DataLoader(
    datasets.CIFAR10(root="data", train=True, download=True, transform=transform),
    batch_size=BATCH_SIZE, shuffle=True
)

gen = Generator(Z_DIM).to(DEVICE)
disc = Discriminator().to(DEVICE)
gen.apply(weights_init)
disc.apply(weights_init)

criterion = nn.BCELoss()
optimizer_gen = optim.Adam(gen.parameters(), lr=LR, betas=(0.5, 0.999))
optimizer_disc = optim.Adam(disc.parameters(), lr=LR, betas=(0.5, 0.999))

fixed_noise = torch.randn(64, Z_DIM, 1, 1).to(DEVICE)

print("ðŸš€ Training Started...")
for epoch in range(NUM_EPOCHS):
    for i, (real, _) in enumerate(dataloader):
        real = real.to(DEVICE)
        batch_size = real.size(0)

        ### Train Discriminator
        noise = torch.randn(batch_size, Z_DIM, 1, 1).to(DEVICE)
        fake = gen(noise)

        disc_real = disc(real).reshape(-1)
        loss_real = criterion(disc_real, torch.ones_like(disc_real))

        disc_fake = disc(fake.detach()).reshape(-1)
        loss_fake = criterion(disc_fake, torch.zeros_like(disc_fake))

        loss_disc = (loss_real + loss_fake) / 2
        disc.zero_grad()
        loss_disc.backward()
        optimizer_disc.step()

        ### Train Generator
        output = disc(fake).reshape(-1)
        loss_gen = criterion(output, torch.ones_like(output))
        gen.zero_grad()
        loss_gen.backward()
        optimizer_gen.step()

        # Print progress
        if i % 100 == 0:
            print(f"Epoch [{epoch}/{NUM_EPOCHS}] Batch {i}/{len(dataloader)} \
                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}")

    # Save generated image samples
    with torch.no_grad():
        fake = gen(fixed_noise).detach().cpu()
    grid = make_grid(fake, nrow=8, normalize=True)
    save_image(grid, f"{SAMPLE_DIR}/epoch_{epoch}.png")

print("âœ… Training Completed! Images saved to /samples/")

# ðŸ“Œ Display all generated images from each epoch
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import glob

# Get list of saved sample images
image_paths = sorted(glob.glob(f"{SAMPLE_DIR}/epoch_*.png"))

# Display images one by one
for path in image_paths:
    img = mpimg.imread(path)
    plt.figure(figsize=(6,6))
    plt.imshow(img)
    plt.axis("off")
    plt.title(f"Generated Image - {os.path.basename(path)}")
    plt.show()

